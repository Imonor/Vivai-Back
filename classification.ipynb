{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#data scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split , KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         sentence intention\n",
      "0    Est ce que j'ai besoin d'arroser ma plante ?  arrosage\n",
      "1               Quand dois-je arroser ma plante ?  arrosage\n",
      "2                     Dois-je arroser ma plante ?  arrosage\n",
      "3             Comment puis-je arroser ma plante ?  arrosage\n",
      "4          Ma plante a besoin de beaucoup d'eau ?  arrosage\n",
      "..                                            ...       ...\n",
      "188                            je suis allergique  maladies\n",
      "189                          risque de maladies ?  maladies\n",
      "190                  comment protéger ma plante ?  maladies\n",
      "191                                   coccinelles  maladies\n",
      "192            dois je utiliser des coccinelles ?  maladies\n",
      "\n",
      "[193 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "data = pd.read_csv(\"TrainingSet1.csv\")\n",
    "\n",
    "print( data )\n",
    "\n",
    "#temporaire pour moins de données\n",
    "#data = data.head()\n",
    "\n",
    "intentions = data[\"intention\"].unique()\n",
    "data[\"intention\"] = data[\"intention\"].replace({\"arrosage\" : np.where( intentions == 'arrosage'),\n",
    "                                              \"soleil\": np.where( intentions == 'soleil'), \n",
    "                                              \"tailler\" : np.where( intentions == 'tailler'),\n",
    "                                              \"temperature\" : np.where( intentions == 'temperature'),\n",
    "                                              \"cadeaux\" : np.where( intentions == 'cadeaux'),\n",
    "                                              \"varietes\" : np.where( intentions == 'varietes'),\n",
    "                                              \"entretien\" : np.where( intentions == 'entretien'),\n",
    "                                               \"utilisation\" : np.where( intentions == 'utilisation'),\n",
    "                                               \"planter\" : np.where( intentions == 'planter'),\n",
    "                                               \"maladies\" : np.where( intentions == 'maladies')\n",
    "                                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(data[\"intention\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = data[\"sentence\"].tolist()\n",
    "j=0;\n",
    "\n",
    "#dictionnaire des mots connus\n",
    "words = list()\n",
    "allowed_pos = ['VERB', 'NOUN', 'PROPN', 'ADJ']\n",
    "\n",
    "# process sentences\n",
    "for i in data[\"sentence\"]:\n",
    "    # convert all letters to lower case\n",
    "    i = i.lower()\n",
    "    i = i.replace('-', ' ')\n",
    "\n",
    "    nlp_fr = spacy.load('fr_core_news_sm')\n",
    "    tokens = nlp_fr(i)\n",
    "\n",
    "    new_sentence = ''\n",
    "    for token in tokens:\n",
    "        if ( allowed_pos.count(token.pos_) > 0 ):\n",
    "            if (token.lemma_ != 'plante' and token.lemma_ != 't'): \n",
    "                new_sentence += str(token.lemma_) + ' '\n",
    "                words.append(str(token.lemma_))\n",
    "        \n",
    "    liste[j] = new_sentence;\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(liste).todense() #renvoie le bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\n",
      "[18]\n",
      "[19]\n",
      "[24]\n",
      "[28]\n",
      "[29]\n",
      "[31]\n",
      "[32]\n",
      "[38]\n",
      "[45]\n",
      "[50]\n",
      "[52]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[59]\n",
      "[60]\n",
      "[62]\n",
      "[63]\n",
      "[67]\n",
      "[68]\n",
      "[74]\n",
      "[77]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[107]\n",
      "[108]\n",
      "[115]\n",
      "[116]\n",
      "[124]\n",
      "[128]\n",
      "[130]\n",
      "[132]\n",
      "[133]\n",
      "[134]\n",
      "[142]\n",
      "[143]\n",
      "[144]\n",
      "[145]\n",
      "[147]\n",
      "[148]\n",
      "[149]\n",
      "[151]\n",
      "[152]\n",
      "[156]\n",
      "[157]\n",
      "[158]\n",
      "[159]\n",
      "[160]\n",
      "[161]\n",
      "[163]\n",
      "[164]\n",
      "[165]\n",
      "[166]\n",
      "[167]\n",
      "[169]\n",
      "[172]\n",
      "[183]\n",
      "[187]\n",
      "[188]\n",
      "[190]\n"
     ]
    }
   ],
   "source": [
    "compt_svm = 0\n",
    "\n",
    "for train_index, test_index in loo.split(X): \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for index in train_index:\n",
    "        X_train+=X[index].tolist()\n",
    "        y_train.append(y[index])\n",
    "    X_test = X[test_index].tolist()\n",
    "    y_test = y[int(test_index)]\n",
    "    \n",
    "    clf_svm = svm.SVC(kernel='linear')\n",
    "    clf_svm.fit(X_train, y_train)\n",
    "            \n",
    "    if int(clf_svm.predict(X_test))==y_test:  # si prediction svm correcte\n",
    "        compt_svm += 1 \n",
    "    else:\n",
    "        print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision & Recall SVM : 0.6735751295336787\n"
     ]
    }
   ],
   "source": [
    "print(\"Précision & Recall SVM :\", compt_svm/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clf_svm.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf_svm, 'clf_svm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant de les charger à nouveau\n",
    "clf_svm = load('clf_svm.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maladies']\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple sentence\n",
    "s = \"Ma plante ne va pas bien\"\n",
    "\n",
    "# Pre-processing\n",
    "s = s.lower()\n",
    "\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "tokens = nlp_fr(s)\n",
    "\n",
    "words = list()\n",
    "                \n",
    "# Lemmatize\n",
    "for token in tokens:\n",
    "    if ( allowed_pos.count(token.pos_) > 0 ):\n",
    "        if (token.lemma_ != 'plante' and token.lemma_ != 't'): \n",
    "            words.append(str(token.lemma_))\n",
    "\n",
    "           \n",
    "j = 0;\n",
    "vector = vectorizer.get_feature_names()\n",
    "\n",
    "# Create vector\n",
    "for word in vector:\n",
    "    vector[j] = words.count(word);           \n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "p = clf_svm.predict([vector])\n",
    "print(intentions[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"intention\"][192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
