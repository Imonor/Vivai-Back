{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "#data scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split , KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence  intention\n",
      "0         Est ce que j'ai besoin d'arroser ma plante ?   arrosage\n",
      "1                    Quand dois-je arroser ma plante ?   arrosage\n",
      "2                          Dois-je arroser ma plante ?   arrosage\n",
      "3                  Comment puis-je arroser ma plante ?   arrosage\n",
      "4               Ma plante a besoin de beaucoup d'eau ?   arrosage\n",
      "..                                                 ...        ...\n",
      "218                       je veux savoir des anecdotes  anecdotes\n",
      "219                            donne moi des anecdotes  anecdotes\n",
      "220  quels sont les trucs cool à savoir sur ma plante?  anecdotes\n",
      "221     dis moi des trucs cool à savoir sur ma plante?  anecdotes\n",
      "222  raconte moi des trucs cool à savoir sur ma pla...  anecdotes\n",
      "\n",
      "[223 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "data = pd.read_csv(\"TrainingSet1.csv\")\n",
    "\n",
    "print( data )\n",
    "\n",
    "#temporaire pour moins de données\n",
    "#data = data.head()\n",
    "\n",
    "intentions = data[\"intention\"].unique()\n",
    "data[\"intention\"] = data[\"intention\"].replace({\"arrosage\" : np.where( intentions == 'arrosage'),\n",
    "                                              \"soleil\": np.where( intentions == 'soleil'), \n",
    "                                              \"tailler\" : np.where( intentions == 'tailler'),\n",
    "                                              \"temperature\" : np.where( intentions == 'temperature'),\n",
    "                                              \"cadeaux\" : np.where( intentions == 'cadeaux'),\n",
    "                                              \"varietes\" : np.where( intentions == 'varietes'),\n",
    "                                              \"entretien\" : np.where( intentions == 'entretien'),\n",
    "                                               \"utilisation\" : np.where( intentions == 'utilisation'),\n",
    "                                               \"planter\" : np.where( intentions == 'planter'),\n",
    "                                               \"maladies\" : np.where( intentions == 'maladies'),\n",
    "                                               \"anecdotes\" : np.where( intentions == 'anecdotes')\n",
    "                                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(data[\"intention\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = data[\"sentence\"].tolist()\n",
    "j=0;\n",
    "\n",
    "#dictionnaire des mots connus\n",
    "words = list()\n",
    "\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# process sentences\n",
    "for i in data[\"sentence\"]:\n",
    "    # convert all letters to lower case\n",
    "    i = i.lower()\n",
    "    i = i.replace('-', ' ')\n",
    "    \n",
    "    regex = re.compile(\"plante([^r]|$)\")\n",
    "    i = regex.sub('', i)\n",
    "    tokens = nlp_fr(i)\n",
    "\n",
    "    new_sentence = ''\n",
    "    for token in tokens:\n",
    "            if (token.lemma_ != 't'): \n",
    "                new_sentence += str(token.lemma_) + ' '\n",
    "                words.append(str(token.lemma_))\n",
    "    \n",
    "    if( new_sentence.strip() != \"\" ):\n",
    "        liste[j] = new_sentence.strip();\n",
    "        j += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(liste).todense() #renvoie le bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compt_svm = 0\n",
    "\n",
    "for train_index, test_index in loo.split(X): \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for index in train_index:\n",
    "        X_train+=X[index].tolist()\n",
    "        y_train.append(y[index])\n",
    "    X_test = X[test_index].tolist()\n",
    "    y_test = y[int(test_index)]\n",
    "    \n",
    "    clf_svm = CalibratedClassifierCV(svm.LinearSVC())\n",
    "    clf_svm.fit(X_train, y_train)\n",
    "\n",
    "    if int(clf_svm.predict(X_test)[0])==y_test:  # si prediction svm correcte\n",
    "         compt_svm += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision SVM : 0.8340807174887892\n"
     ]
    }
   ],
   "source": [
    "print(\"Précision SVM :\", compt_svm/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf_svm, 'clf_svm.joblib')\n",
    "dump(intentions, 'intentions.joblib')\n",
    "dump(vectorizer, 'vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant de les charger à nouveau\n",
    "clf_svm = load('clf_svm.joblib') \n",
    "intentions = load('intentions.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anecdotes\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple sentence\n",
    "s = \"j'aimerais entendre une anecdote\"\n",
    "\n",
    "# Pre-processing\n",
    "s = s.lower()\n",
    "\n",
    "regex = re.compile(\"plante([^r]|$)\")\n",
    "s = regex.sub('', s)\n",
    "\n",
    "nlp_fr = \"\";\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "\n",
    "tokens = nlp_fr(s)\n",
    "\n",
    "words = list()\n",
    "                \n",
    "# Lemmatize\n",
    "for token in tokens:\n",
    "    if (token.lemma_ != 't'): \n",
    "        words.append(str(token.lemma_))\n",
    "\n",
    "           \n",
    "j = 0;\n",
    "vector = vectorizer.get_feature_names()\n",
    "\n",
    "# Create vector\n",
    "for word in vector:\n",
    "    vector[j] = words.count(word);           \n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "p = clf_svm.predict([vector])\n",
    "print(intentions[ int(p[0]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf_svm.predict_proba([vector])\n",
    "best = score[0][int(p[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02170843, 0.0565371 , 0.0130779 , 0.03514684, 0.1095046 ,\n",
       "        0.03726466, 0.04098199, 0.00710449, 0.03566888, 0.05523033,\n",
       "        0.58777479]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.delete(score, np.where(score == best))\n",
    "second_best = np.amax(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8560301760252815"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math  \n",
    "1/(1+math.exp(-(-0.3+(best-second_best))*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
