{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "#data scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split , KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         sentence intention\n",
      "0    Est ce que j'ai besoin d'arroser ma plante ?  arrosage\n",
      "1               Quand dois-je arroser ma plante ?  arrosage\n",
      "2                     Dois-je arroser ma plante ?  arrosage\n",
      "3             Comment puis-je arroser ma plante ?  arrosage\n",
      "4          Ma plante a besoin de beaucoup d'eau ?  arrosage\n",
      "..                                            ...       ...\n",
      "209                            je suis allergique  maladies\n",
      "210                          risque de maladies ?  maladies\n",
      "211                  comment protéger ma plante ?  maladies\n",
      "212                                   coccinelles  maladies\n",
      "213            dois je utiliser des coccinelles ?  maladies\n",
      "\n",
      "[214 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "data = pd.read_csv(\"TrainingSet1.csv\")\n",
    "\n",
    "print( data )\n",
    "\n",
    "#temporaire pour moins de données\n",
    "#data = data.head()\n",
    "\n",
    "intentions = data[\"intention\"].unique()\n",
    "data[\"intention\"] = data[\"intention\"].replace({\"arrosage\" : np.where( intentions == 'arrosage'),\n",
    "                                              \"soleil\": np.where( intentions == 'soleil'), \n",
    "                                              \"tailler\" : np.where( intentions == 'tailler'),\n",
    "                                              \"temperature\" : np.where( intentions == 'temperature'),\n",
    "                                              \"cadeaux\" : np.where( intentions == 'cadeaux'),\n",
    "                                              \"varietes\" : np.where( intentions == 'varietes'),\n",
    "                                              \"entretien\" : np.where( intentions == 'entretien'),\n",
    "                                               \"utilisation\" : np.where( intentions == 'utilisation'),\n",
    "                                               \"planter\" : np.where( intentions == 'planter'),\n",
    "                                               \"maladies\" : np.where( intentions == 'maladies')\n",
    "                                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(data[\"intention\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = data[\"sentence\"].tolist()\n",
    "j=0;\n",
    "\n",
    "#dictionnaire des mots connus\n",
    "words = list()\n",
    "allowed_pos = ['VERB', 'NOUN', 'PROPN', 'ADJ']\n",
    "\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# process sentences\n",
    "for i in data[\"sentence\"]:\n",
    "    # convert all letters to lower case\n",
    "    i = i.lower()\n",
    "    i = i.replace('-', ' ')\n",
    "    \n",
    "    regex = re.compile(\"plante([^r]|$)\")\n",
    "    i = regex.sub('', i)\n",
    "    tokens = nlp_fr(i)\n",
    "\n",
    "    new_sentence = ''\n",
    "    for token in tokens:\n",
    "        if ( allowed_pos.count(token.pos_) > 0 ):\n",
    "                if (token.lemma_ != 't'): \n",
    "                    new_sentence += str(token.lemma_) + ' '\n",
    "                    words.append(str(token.lemma_))\n",
    "    \n",
    "    if( new_sentence.strip() != \"\" ):\n",
    "        liste[j] = new_sentence.strip();\n",
    "        j += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(liste).todense() #renvoie le bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compt_svm = 0\n",
    "\n",
    "for train_index, test_index in loo.split(X): \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for index in train_index:\n",
    "        X_train+=X[index].tolist()\n",
    "        y_train.append(y[index])\n",
    "    X_test = X[test_index].tolist()\n",
    "    y_test = y[int(test_index)]\n",
    "    \n",
    "    clf_svm = CalibratedClassifierCV(svm.LinearSVC())\n",
    "    clf_svm.fit(X_train, y_train)\n",
    "\n",
    "    if int(clf_svm.predict(X_test)[0])==y_test:  # si prediction svm correcte\n",
    "         compt_svm += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision SVM : 0.7570093457943925\n"
     ]
    }
   ],
   "source": [
    "print(\"Précision SVM :\", compt_svm/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf_svm, 'clf_svm.joblib')\n",
    "dump(intentions, 'intentions.joblib')\n",
    "dump(vectorizer, 'vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant de les charger à nouveau\n",
    "clf_svm = load('clf_svm.joblib') \n",
    "intentions = load('intentions.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planter\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple sentence\n",
    "s = \"j'aimerais offrir une plante à ma m\"\n",
    "\n",
    "# Pre-processing\n",
    "s = s.lower()\n",
    "\n",
    "regex = re.compile(\"plante([^r]|$)\")\n",
    "s = regex.sub('', s)\n",
    "\n",
    "nlp_fr = \"\";\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "\n",
    "tokens = nlp_fr(s)\n",
    "\n",
    "words = list()\n",
    "                \n",
    "# Lemmatize\n",
    "for token in tokens:\n",
    "    if ( allowed_pos.count(token.pos_) > 0 ):\n",
    "        if (token.lemma_ != 't'): \n",
    "            words.append(str(token.lemma_))\n",
    "\n",
    "           \n",
    "j = 0;\n",
    "vector = vectorizer.get_feature_names()\n",
    "\n",
    "# Create vector\n",
    "for word in vector:\n",
    "    vector[j] = words.count(word);           \n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "p = clf_svm.predict([vector])\n",
    "print(intentions[ int(p[0]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf_svm.predict_proba([vector])\n",
    "best = score[0][int(p[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.delete(score, np.where(score == best))\n",
    "second_best = np.amax(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9263150636726196"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math  \n",
    "1/(1+math.exp(-(-0.3+(best-second_best))*10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
